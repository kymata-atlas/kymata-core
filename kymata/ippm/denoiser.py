from copy import deepcopy
from statistics import NormalDist
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN as DBSCAN_, MeanShift as MeanShift_
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import normalize
from sklearn.utils._testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning

#import multiprocessing

from .data_tools import IPPMHexel


class DenoisingStrategy(object):
    """
        Interface for unsupervised clustering algorithms. Strategies should conform to this interface.
    """
    def __init__(self, **kwargs):
        """
            kwargs varies from strategy to strategy. The subclasses should be aware of the important ones and set default values if
            there is no value supplied for it. Although sklearn has guardrails for input params, some assertions are also provided.
        """
        self._clusterer = None
    
    def cluster(
        self, hexels: Dict[str, IPPMHexel], hemi: str, normalise: bool = False, cluster_latency: bool = False,
        posterior_pooling: bool = False
    ) -> Dict[str, IPPMHexel]:
        """
            For each function in hemi, it will attempt to construct a dataframe that holds significant spikes (i.e., abova alpha).
            Next, we do any optional preprocessing and cluster using self._clusterer. Finally, it locates the minimum 
            (most significant) point for each cluster and saves it. Optionally, the user can choose to constrain the number of spikes to 1.
            Preprocessing includes scaling the data to have unit length or cluster only on latency (Density based clustering).

            This can be overridden if using a custom clustering strategy but as it is, it works well sklearn clustering techniques. As a result,
            additional algorithms from sklearn can be easily incorporated.

            Essentially the algorithm operates by tagging each cluster as being generated by one original spike. Our algorithm attempts 
            to delineate the clusters and assumes the most significant spike is the original generating spike.
            
            Params
            ------
            hexels : Dict[str, Hexel]
                     Contains the left hemisphere and right hemisphere pairings. We want to denoise one of them.
            hemi : str from ['rightHemisphere', 'leftHemisphere']
                   indicates the hemisphere to denoise.
            normalise: bool
                       maps vectors to unit vectors. I.e., divides by the total length rather than standardise (map to N(0, 1)).
                       normalisation can aid algorithms using distance metrics, e.g., euclidean distance. Intuitively, having unnormalised
                       dimensions gives a heftier weight to the larger one, in this case, latency. By normalising, each dimension contributes the 
                       same amount to the overall algorithm. Tests showed that normalising does not improve performance.
            cluster_latency: bool
                             indicates whether to cluster only on the latency dimension. It is akin to having unnormalised features because
                             the latency dimension has a far higher weight than the magnitude dimension, latency is from -200 to 800 while magnitude
                             is 10^-x which is incredibly small. Clustering solely on latency can actually boost performance in some cases. However,
                             it is the same as a density-based algorithm because we map the problem to 1D and cluster on the density of points in
                             time. Hence, we keep it by default to False, so not all algorithms default to a density-based one.
            posterior_pooling: bool
                               indicates whether we want to pool at the end of denoising to constrain the number of nodes per function to be 1.
                               it returns graphs of increased transparency at the price of enforcing a constraint, which may be false. The 
                               assumption is that each function only appears once. To keep the results as close to the Truth as possible, it is set
                               by default to 1 but can be set to True to generate nice graphs.

            Returns
            -------
            A new dictionary that has the denoised pairings for each function. Same format as input dict hexels.
            
        """
        self._check_hemi(hemi) # guardrail to check for hemisphere input.
        hexels = deepcopy(hexels) # dont do it in-place.
        
        alpha = self._estimate_alpha()
        for func, df in self._hexels_to_df(hexels, hemi, alpha):
            # Step 1: For each time series, we cluster on a dataframe with only significant spikes.
            if len(df) == 0:
                # there are no significant spikes.
                hexels = self._update_pairings(hexels, func, [], hemi)
                continue

            # Step 2: Perform any preprocessing and run the clusterer.
            # if we are renormalising each feature, then normalise otherwise no
            if cluster_latency:
                # cluster only the latency dimension. normalising a 1d dataset should make no difference but kept it incase
                latency_only = self._get_latency_dim(df)
                fitted = (self._clusterer.fit(latency_only)
                          if not normalise else 
                          self._clusterer.fit(normalize(latency_only)))
            else:
                fitted = self._clusterer.fit(normalize(df)) if normalise else self._clusterer.fit(df)

            # Step 3: Identify clusters and extract the most significant spike.
            df['Label'] = fitted.labels_
            cluster_mins = self._get_cluster_mins(df)

            # Step 4: Overwrite the noisy pairings and optionally max pool
            hexels = self._update_pairings(hexels, func, cluster_mins, hemi)
            if posterior_pooling:
                hexels = self._posterior_pooling(hexels, hemi, func)
        return hexels

    """
        This commented region contains the code for the multiprocessing version of the denoiser. 
        Currently, it is commented out because it does not offer an improvement in performance due to the
        overhead cost of managing the processes outweighing the boost in speed. As the dataset size increases,
        the value of multiprocessing code will increase, so eventually, this code might be useful. Therefore, it is
        kept as a reference point.

        Also, Python is quite poor at concurrency. Python is compiled to bytecode, which is run by an interpreter. However, there is only
        one shared interpreter across processes; hence, only one process runs at a time. So, Python gives off the illusion of concurrency when
        it is actually just context switching rapidly between jobs. Incorporate the overhead of managing the threads/processes and concurrent
        Python code can actually get slower.

    
    def cluster_multiproc(self, hexels, hemi, normalise = False, cluster_latency = False, posterior_pooling = False):
        self._check_hemi(hemi)
        hexels = deepcopy(hexels)
        alpha = self._estimate_alpha()
        with multiprocessing.Pool() as pool:
            args = []
            for func, df in self._hexels_to_df(hexels, hemi, alpha):
                # aggregate independent dfs for each time series we want to cluster on.
                if len(df) == 0:
                    # don't save this as no point.
                    hexels = self._update_pairings(hexels, func, [], hemi)
                else:
                    args.append((df, func, normalise, cluster_latency, posterior_pooling))

            # cluster each one in parallel
            # we need to create the dataframes independently because multiprocessing copies by reference (so if you pass hexels, it copies it f times.)
            result = pool.starmap(self._cluster_worker, args)

            # update hexels with new minimums.
            for func, cluster_mins in result:
                hexels = self._update_pairings(hexels, func, cluster_mins, hemi)    
        return hexels if not posterior_pooling else self._posterior_pooling(hexels, hemi)

    def _cluster_worker(self, df, func, normalise, cluster_latency, posterior_pooling):
        if cluster_latency:
            latency_only = self._get_latency_dim(df)
            fitted = (self._clusterer.fit(latency_only)
                      if not normalise else 
                      self._clusterer.fit(normalize(latency_only)))
        else:
            fitted = self._clusterer.fit(normalize(df)) if normalise else self._clusterer.fit(df)
        df['Label'] = fitted.labels_
        cluster_mins = self._get_cluster_mins(df)
        return (func, cluster_mins)
    """

    def _get_latency_dim(self, df: pd.DataFrame) -> np.ndarray:
        """
            Utility function to map latency from a 1D array to a 2D array of form: [[latency_elem_1, ..., latency_elem_n]]
            Used when clustering solely on latency.

            Params
            ------
            df: dataframe
                dataframe contains Latency column, which we want to turn into a 2D np array.

            Returns
            -------
            2D np array containing the latency column from df.
        """
        return np.reshape(df['Latency'], (-1, 1))

    def _posterior_pooling(self, hexels: Dict[str, IPPMHexel], hemi: str, func: str) -> Dict[str, IPPMHexel]:
        """
            Optional max pooling over the entire latency done at the end of clustering. It enforces the constraint that there is only one
            spike per function. 

            Params
            ------
            hexels: dict of func_name, hexel
                    each hexel contains a list of cluster minimums. We wish to take the min over this list
            hemi: either 'rightHemisphere' or 'leftHemisphere'. we dont check because the parent function checks.
                  which hemi we are clustering over.
            func: the function we want to max pool over.

            Returns
            -------
            the same dictionary but now one of the hemispheres has its list reduced to a single spike.
        """
        
        if len(hexels[func].left_best_pairings) != 0 and hemi == 'leftHemisphere':
            hexels[func].left_best_pairings = [min(hexels[func].left_best_pairings, key=lambda x: x[1])]
        elif len(hexels[func].right_best_pairings) != 0 and hemi == 'rightHemisphere':
                hexels[func].right_best_pairings = [min(hexels[func].right_best_pairings, key=lambda x: x[1])]
        return hexels
    
    def _hexels_to_df(self, hexels: Dict[str, IPPMHexel], hemi: str, alpha: float) -> pd.DataFrame:
        """
            A generator used to build a dataframe of significant points only. For each call, it returns the dataframe
            for the next function in hexels.keys().

            Params
            ------
            hexels : dict[str, Hexel]
                     A dictionary with function names as keys and hexel objects containing the pairings. 
            hemi : str from ['rightHemisphere', 'leftHemisphere']

            Returns
            -------
            A dataframe for a function that contains only significant spikes, i.e., those above alpha.
        """
        for func in hexels.keys():
            df = pd.DataFrame(columns=['Latency', 'Mag'])
            df = self._filter_spikes(hexels[func].right_best_pairings if hemi == 'rightHemisphere' else
                                     hexels[func].left_best_pairings,
                                     df, 
                                     alpha)
            yield (func, df)

    def _filter_spikes(self, spikes: List[Tuple[float, float]], df: pd.DataFrame, alpha: float) -> pd.DataFrame:
        """
            Use this to filter out all insignificant spikes and add significant spikes to a dataframe.

            Params
            ------
            spikes : List[float, float]
                     the pairings from either the left or right hemisphere. Collection of (latency, magnitude) pairs.
            df : pd.DataFrame
                 A dataframe to which we want to add the significant spikes. It has columns [Latency, Mag].
            alpha : float
                    The threshold for significance.

            Returns
            -------
            the df parameter but populated with significant spikes.
        """
        for latency, spike in spikes:
            if spike <= alpha:
                #significant
                df.loc[len(df)] = [latency, spike]
        return df

    def _estimate_alpha(self) -> float:
        """
            Fetch the threshold of significance. Currently hard-coded but can be updated to take params. IF custom one is required,
            it might be better to move estimate alpha to data_utils and precompute alpha and pass as a variable to clustering algos.

            Returns
            -------
            A float indicating the magnitude that a spike needs to be smaller than for it to be significant.
        """
        timepoints = 201
        number_of_hexels = 200000
        alpha = 1 - NormalDist(mu=0, sigma=1).cdf(5)      # 5-sigma
        bonferroni_corrected_alpha = 1-(pow((1-alpha),(1/(2*timepoints*number_of_hexels))))
        return bonferroni_corrected_alpha
    
    def _check_hemi(self, hemi):
        """
            A guardrail function to verify the hemi input is one of rightHemisphere or leftHemisphere.

            Params
            ------
            hemi : str
                   should be rightHemisphere or leftHemisphere
            
            Throws
            ------
            ValueError if invalid hemi input.
        """
        if hemi not in ['rightHemisphere', 'leftHemisphere']:
            print('Hemisphere needs to be rightHemisphere or leftHemisphere')
            raise ValueError
    
    def _update_pairings(self, hexels: Dict[str, IPPMHexel], func: str, denoised: List[Tuple[float, float]], hemi: str) -> Dict[str, IPPMHexel]:
        """
            Overwrite the previous pairings with the new denoised version. 

            Params
            ------
            hexels : Dict[func_name, Hexel]
                     Where we want to save the new denoised pairings.
            func : a key for hexels
                   Indicates the function we are saving for. Use it to index into hexels.
            denoised : List[(latency, magnitude)]
                       The denoised pairings. 
            hemi : rightHemisphere or leftHemisphere
                   the hemisphere we are denoising over.

            Returns
            -------
            denoised hexels.
        """
        if hemi == 'rightHemisphere':
            hexels[func].right_best_pairings = denoised
        else:
            hexels[func].left_best_pairings = denoised
        return hexels

    def _get_cluster_mins(self, df: pd.DataFrame) -> List[Tuple[float, float]]:
        """
            Use this to construct a list of tuples that contain the most significant (minimum) spikes for each class.
            Note: assume that a label of -1 is an anomaly. Some of the algorithms in sklearn can identify outliers, so it is a useful assumption.

            Params
            ------
            df : pd.DataFrame
                 It needs to have a column called labels, which contains the cluster assignment for each data point.
            
            Returns
            -------
            For each class, the most significant spike and the associated latency.
        """
        mins = df.loc[df.groupby('Label')['Mag'].idxmin()]
        mins = mins[mins['Label'] != -1] # filter out anomalies
        return list(zip(mins['Latency'], mins['Mag']))
    
class MaxPooler(DenoisingStrategy):
    """
        Naive max pooling technique. It operates by first sorting the latencies into bins, identifying significant bins, and taking the 
        most significant spike in a significant bin. A bin is considered significant if the number of spikes for a particular function 
        in the bin exceeds the threshold (self._threshold). Moreover, the bin size can be controlled by the bin size (self._bin_sz) hyperparameter. 
        To improve robustness, the threshold should be increased or bin size reduced. A criteria that is too stringent may lead to no 
        significant spikes leading to low function recall, so it should be balanced. Finally, it is possible to run max pooler as an anomaly 
        detection system prior to running an unsupervised algorithm, albeit at a higher computational cost.
    """
    
    def __init__(self, **kwargs):
        """
            Params
            ------
            threshold : int
                        # of spikes required in a bin before it is considered significant
            bin_sz : int
                     the size, in ms, of a bin.
        """

        self._threshold = 15 if 'threshold' not in kwargs.keys() else kwargs['threshold']
        self._bin_sz = 25 if 'bin_sz' not in kwargs.keys() else kwargs['bin_sz']

        
        if not isinstance(self._threshold, int) or isinstance(self._threshold, bool):
            # edge case with isinstance: bool is subtype of int, so technically a user can pass in a boolean value for an integer
            # True = 1, False = 0.
            print('Threshold needs to be an integer.')
            raise ValueError

        if not isinstance(self._bin_sz, int) or isinstance(self._bin_sz, bool):
            print('Bin size needs to be an integer.')
            raise ValueError

    def cluster(
        self, hexels: Dict[str, IPPMHexel], hemi: str, normalise: bool = False,
        cluster_latency: bool = False, posterior_pooling: bool = False
    ) -> Dict[str, IPPMHexel]:
        """  
            Custom clustering method since it differs from other unsupervised techniques. 
            
            Yikes: forgot to incorporate cluster_bin
            logn_f + 1 > # of bins is greater.

            Time Complexity: O(f * (nlogn + n)) where n = number of pairings = # of hexels = 10000. Final: O(f * nlogn)
            Time Complexity (if you don't sort): O(f * (# of bins * n)) where # of bins = 1000/binsz = 1000/25 = 40. Final: O(f * bin_num * n)
            Hence, if bin_num > logn, then we should go for sorted. bin_num = 40 assuming 1000 latency range 25 bin size. 
            log10000 = logn = 4. Therefore, we should go with sorted.

            Possible optimisation: break it up into parallel and do each bin at once? Ans: Python bad

            Algorithm
            ---------
            for each func:
                sort by latency, so we can partition into bins in ascending order. If it is unordered, then there is no guarentee that adjacent data points belong to the same bin.
                ret = []
                for current_bin in partitioned_latency_axis:
                    number_of_spikes := 0
                    most_significant := infinity (the most significant are the ones with the smallest magnitude)
                    latency := null
                    for (current_data_point, current_latency) in current_bin
                        number_of_spikes++
                        if current_data_point < most_significant:
                            most_significant = current_data_point
                            latency = current_latency
                        
                    if number_of_spikes > threshold:
                        ret.append((latency, most_significant))
                return ret                    

            Params
            ------
            see DenoisingStrategy.    

            Returns
            -------
            see DenoisingStrategy.
        """
        super()._check_hemi(hemi)
        hexels = deepcopy(hexels)
        alpha = self._estimate_alpha()
        for func, df in super()._hexels_to_df(hexels, hemi, alpha):
            if len(df) == 0:
                hexels = super()._update_pairings(hexels, func, [], hemi)
                continue

            df = df.sort_values(by='Latency') # arrange latencies into bins. It uses QuickSort. Complexity: O(nlogn) worst case: O(n^2)
            r_idx = 0                         # this points to the current_data_point. It is incremented in the inner loop.
            ret = []
            for latency in range(-200, 800, self._bin_sz):
                # latency indicates the start of the current bin. E.g., latency = -200 means the bin is [-200, -200 + self._bin_sz)
                if r_idx >= len(df):
                    # no data left.
                    break

                bin_min, lat_min, num_seen, r_idx = self._cluster_bin(df, r_idx, latency)

                if bin_min != np.inf and num_seen >= self._threshold:
                    # significant bin, so save the cluster mins.
                    ret.append((lat_min, bin_min))
            
            hexels = super()._update_pairings(hexels, func, ret, hemi)
            if posterior_pooling:
                hexels = self._posterior_pooling(hexels, hemi, func)

        return hexels
    
    def _cluster_bin(self, df: pd.DataFrame, r_idx: int, latency: int) -> Tuple[float, int, int, int]:
        """
            We dont need to check r_idx and latency since cluster function provides them rather than the user.

            Time complexity: O(n_b) where n_b = # of spikes in bin. Don't think we can reduce this further.

            Params
            ------
            df : pd.DataFrame
                 Holds the data about a cluster, specifically, the latency and magnitude.
            r_idx : int
                    Index into the dataframe. Initially, it points to the start of the bin. Since df is ordered by latency, we just need to increment to get next.
            latency : int
                      latency is the current start of bin latency. E.g., latency = 100 means that we are looping over the bin [100, 100 + bin_sz) and r_idx points to the first element in the bin.

            Returns
            --------
            A tuple with format: (minimum magnitude of this bin, the associated latency with the magnitude, number of significant spikes within this bin, the r_idx pointing to the next bin).
        """

        bin_min = float('inf')
        lat_min = None
        num_seen = 0

        if r_idx >= len(df):
            return bin_min, lat_min, num_seen, r_idx

        while latency <= df.iloc[r_idx, 0] < latency + self._bin_sz:
             # loop while the r_idx lands in the current_bin. Once r_idx points to a data point with latency outside of the bin, we break the loop.
            mag = df.iloc[r_idx, 1]
            num_seen += 1
            if mag < bin_min:
                bin_min = mag
                lat_min = df.iloc[r_idx, 0]

            r_idx += 1
            if r_idx >= len(df):
                break
        
        return bin_min, lat_min, num_seen, r_idx

class AdaptiveMaxPooler(DenoisingStrategy):
    def __init__(self, **kwargs):
        """
            Params
            ------
            threshold : int
                        # of spikes required in a bin before it is considered significant
            bin_sz : int
                     the size, in ms, of a bin.
        """
        self._threshold = 5 if 'threshold' not in kwargs.keys() else kwargs['threshold']
        self._base_bin_sz = 10 if 'base_bin_sz' not in kwargs.keys() else kwargs['base_bin_sz']
        
        if not isinstance(self._threshold, int) or isinstance(self._threshold, bool):
            print('Threshold needs to be an integer.')
            raise ValueError
        if not isinstance(self._base_bin_sz, int) or isinstance(self._base_bin_sz, bool):
            print('Base Bin size needs to be an integer.')
            raise ValueError
        
    def cluster(
        self, hexels: Dict[str, IPPMHexel], hemi: str, normalise: bool = False,
        cluster_latency: bool = False, posterior_pooling: bool = False
    ) -> Dict[str, IPPMHexel]:
        """
            Time complexity is O(f(nlogn + n)). f = # of funcs, n = # of hexels/spikes = 10000. Final: O(f * nlogn)

            If we want to do it in one pass of the dataframe, i.e., in time n, we need to have it sorted. Hence, the nlogn is irreducible since
            sort uses QuickSort under the hood. Also, the f is irreducible unless we don't want to go through each function.

            Adaptive Max Pooler (AMP) is an improvement over the MaxPooler (MP) algorithm. Whereas MP has a fixed bin size, 
            hence a fixed cluster size, AMP starts off with a minute bin size and iteratively expands it until encountering the end of a 
            cluster. Hence, AMP has a variable bin size. Specifically, it works as follows:

            Algorithm
            ---------
            Note: I think the code + comments is probably better to understand algo than this.
            
            Loop through the time-series for each function:
            
                sorted_df = sort(time_series_func)
                
                bins = segment(sorted_df) # break into bins of size 5 ms.

                prev_sig = False
                prev_bin_min = inf
                prev_lat_min = None
                denoised_pairings = []
                for bin in bins:
                    cur_bin_min = inf
                    cur_lat_min = None
                    num_spikes = 0

                    for lat, mag in bin:
                        num_spikes++
                        if mag < cur_bin_min:
                            cur_bin_min = mag
                            cur_lat_min = lat

                    if num_spikes > self._threshold:
                        prev_sig = True
                        if cur_bin_min < prev_bin_min:
                            # it is minimum over prev bin and cur bin
                            prev_bin_min, prev_lat_min = cur_bin_min, cur_lat_min

                    else:
                        # not significant.
                        if prev_sig:
                            # previous bin was significant. This is the case when we reach end of cluster/bin.
                            denoised_pairings.append((prev_bin_min, prev_lat_min))
                            prev_bin_min = inf
                            prev_lat_min = None
                            prev_sig = False

                if prev_sig:
                    # previous bin was significant. This is the case when we reach end of dataframe and the final bin is significant.
                    denoised_pairings.append((prev_bin_min, prev_lat_min))
                    prev_bin_min = inf
                    prev_lat_min = None
                    prev_sig = False

            Parameters
            ----------
            See DenoisingStrategy

            Returns
            -------
            See DenoisingStrategy
        """
        
        def get_default_vals():
            return (np.inf, None)
        
        hexels = deepcopy(hexels)
        alpha = self._estimate_alpha()
        for func, df in super()._hexels_to_df(hexels, hemi, alpha):
            # Step 1: Create a dataframe for each function with only significant spikes
            if len(df) == 0:
                # dont cluster on an empty dataframe
                hexels = super()._update_pairings(hexels, func, [], hemi)
                continue

            # Step 2: Sort the pairings so that we can loop through them in one go.
            df = df.sort_values(by='Latency')


            # Step 3: Loop through and expand significant bins until there aren't anymore significant ones. Take the min over the expanded bin.
            df_ptr = 0                                          # Pointer to where we are in the dataframe.
            end_ptr = 1                                         # Delineates the end of a bin
            start_ptr = 0                                       # Delineates the start of a bin
            total_bins = 1000 / self._base_bin_sz               # # of bins. Assumption: We keep latency in [-200, 800].
            prev_bin_min, prev_bin_lat_min = get_default_vals() # bin_min = np.inf, lat_min = None. Default vals when no datapoints in bin.
            prev_signi = False                                  # signifies whether the previous bin was significant. If so, we expand into curr bin
            ret = []                                            # Denoised hexel pairings.
            while df_ptr < len(df) and start_ptr < total_bins:
                # Loop until we end of dataframe or exceed the number of bins
                
                end_ms = end_ptr * self._base_bin_sz
                num_in_bin = 0
                cur_bin_min, cur_bin_lat_min = get_default_vals()
                while df_ptr < len(df) and df.iloc[df_ptr, 0] < end_ms:
                    # Step 3.1) Loop through the bin and take the min over significant spikes
                    if df.iloc[df_ptr, 1] < cur_bin_min:
                        # Found significant point, i.e., cur_min > df[df_ptr]['Mag']
                        cur_bin_min, cur_bin_lat_min = df.iloc[df_ptr, 1], df.iloc[df_ptr, 0]
                    num_in_bin += 1
                    df_ptr += 1
                    
                if num_in_bin >= self._threshold:
                    # Step 3.2) Check to see if curr bin is significant, i.e., # of data points > threshold. If so, we save the minimum.
                    prev_signi = True
                    if cur_bin_min < prev_bin_min:
                        # If it is significant, save the minimum over this bin. We will compare it to the next minimum (if it is also significant).
                        prev_bin_min, prev_bin_lat_min = cur_bin_min, cur_bin_lat_min
                else:
                    # Not a significant bin. Either previous bin was not significant (so no cluster) or it was (so we reached the end of a cluster)
                    if prev_signi:
                        # If the previous bin was significant, we take the final minimum (prev_min) and save it. Now we reset the minimums.
                        ret.append((prev_bin_lat_min, prev_bin_min))
                        prev_bin_min, prev_bin_lat_min = get_default_vals()
                        prev_signi = False

                    # Increment start of current bin.
                    start_ptr = end_ptr

                # Proceed to next bin.
                end_ptr += 1
                    
            if prev_signi:
                # The final bin was significant. Due to the algo only looping til last bin, we can miss clustering the final bin.
                # Hence, we do it now.
                ret.append((prev_bin_lat_min, prev_bin_min))
                prev_bin_min, prev_bin_lat_min = get_default_vals()
                prev_signi = False

            # Step 4: Overwrite previous noisy hexels
            hexels = super()._update_pairings(hexels, func, ret, hemi)

            # Optional Step 5: Take maximum over the function to constrain 1 spike.
            if posterior_pooling:
                hexels = self._posterior_pooling(hexels, hemi, func)
        return hexels

class GMM(DenoisingStrategy):
    """
        This strategy uses the GaussianMixtureModel algorithm. It attempts to fit a multimodal Gaussian distribution to the data using the 
        EM algorithm. The primary disadvantage of this model is that the number of Gaussians have to be prespecified. This implementation 
        does a grid search from 1 to max_gaussians to find the optimal number of Gaussians. Moreover, it does not work well with anomalies.
    """
    def __init__(self, **kwargs):
        """
            Params
            ------
            For more details, check https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture
            max_gaussians : int
                            the upper bound on the number of Gaussians to search. 
            covariance_type : str
                              the constraints on the covariance matrix. 'full' means a full covariance matrix.
            max_iter : int
                       the maximum number of EM steps before optimisation terminates.
            n_init : int
                     the number of initialisations (reruns) to do for each max_gaussian. GMM is highly dependent on the initialisations, so it makes sense to do multiple and take the best.
            init_params : str
                          the algorithm used to initialise the parameters. 'k-means++' is an improved version of k-means.
            random_state : int
                           set this if you want your results to be reproducible.
        """
        # we are instantiating multiple models, so save hyperparameters instead of clusterer object.
        self._max_gaussians = 5 if 'max_gaussians' not in kwargs.keys() else kwargs['max_gaussians']
        self._covariance_type = 'full' if 'covariance_type' not in kwargs.keys() else kwargs['covariance_type']
        self._max_iter = 1000 if 'max_iter' not in kwargs.keys() else kwargs['max_iter']
        self._n_init = 8 if 'n_init' not in kwargs.keys() else kwargs['n_init']
        self._init_params = 'kmeans' if 'init_params' not in kwargs.keys() else kwargs['init_params']
        self._random_state = None if 'random_state' not in kwargs.keys() else kwargs['random_state']
        self._is_aic = False if 'is_aic' not in kwargs.keys() else kwargs['is_aic'] # default is BIC since it is better for explanatory models, since it assumes reality lies within the hypothesis space.

        invalid = False
        if not isinstance(self._max_gaussians, int) or isinstance(self._max_gaussians, bool):
            print('Max Gaussians must be of type int.')
            invalid = True
        if self._covariance_type not in ['full', 'tied', 'diag', 'spherical']:
            print('Covariance type must be one of {full, tied, diag, spherical}')
            invalid = True
        if not isinstance(self._max_iter, int) or isinstance(self._max_iter, bool):
            print('Max iterations must be of type int.')
            invalid = True
        if not isinstance(self._n_init, int) or isinstance(self._n_init, bool):
            print('Number of initialisations must be of type int.')
            invalid = True
        if self._init_params not in ['kmeans', 'k-means++', 'random', 'random_from_data']:
            print('Initalisation of parameter strategy must be one of {kmeans, k-means++, random, random_from_data}')
            invalid = True
        if self._random_state is not None and not isinstance(self._random_state, int) or isinstance(self._random_state, bool):
            print('Random state must be none or int.')
            invalid = True
        
        if invalid:
            raise ValueError

    @ignore_warnings(category=ConvergenceWarning)
    def cluster(
        self, hexels: Dict[str, IPPMHexel], hemi: str, normalise: bool = False,
        cluster_latency: bool = False, posterior_pooling: bool = False
    ) -> Dict[str, IPPMHexel]:
        """
            Overriding the superclass cluster function because we want to perform a grid-search over the number of clusters to locate the optimal one.
            It works similarly to the superclass.cluster method but it performs it multiple times. It stops if the number of data points < number of clusters as
            it will not work. 

            Params
            ------
            See DenoisingStrategy()

            Returns
            -------
            See DenoisingStrategy()
        """
        super()._check_hemi(hemi)
        hexels = deepcopy(hexels)
        alpha = self._estimate_alpha()
        for func, df in super()._hexels_to_df(hexels, hemi, alpha):
            # Step 1: Loop through every function time-series in a dataframe format. Each one only contains significant spikes.
            if len(df) == 0:
                hexels = super()._update_pairings(hexels, func, [], hemi)
                continue
                
            if len(df) == 1:
                # no point clustering, just return the single data point.
                ret = [(df.iloc[0, 0], df.iloc[0, 1])]
                hexels = super()._update_pairings(hexels, func, ret, hemi)
                continue

            best_labels = None
            best_score = np.inf 
            for n in range(1, self._max_gaussians):
                # Step 2: Perform a grid-search for the optimal value of n_gaussians (the modality of the mixture of Gaussians)
                if n > len(df):
                    # the number of gaussians has to be less than the number of datapoints.
                    continue

                # Optional Step 3: Do any preprocessing on the dataframe.
                temp = None
                if normalise and cluster_latency:
                    temp = np.reshape(normalize(df['Latency']), (-1, 1))
                if not normalise and cluster_latency:
                    temp = np.reshape(df['Latency'], (-1, 1))
                if normalise and not cluster_latency:
                    temp = normalize(df)
                else:
                    temp = df

                # Step 4: Fit the model
                gmm = GaussianMixture(n_components=n, 
                                      covariance_type=self._covariance_type,
                                      max_iter=self._max_iter,
                                      n_init=self._n_init,
                                      init_params=self._init_params,
                                      random_state=self._random_state)
                gmm.fit(temp)

                # Step 5: Evaluate the model and retain the best performing one.
                score = gmm.aic(temp) if self._is_aic else gmm.bic(temp)
                labels = gmm.predict(temp)
                if score < best_score:
                    # this condition depends on the choice of AIC/BIC
                    best_labels = labels
                    best_score = score

            # Step 6: Update the hexels with the best clustering we found.
            df['Label'] = best_labels
            cluster_mins = super()._get_cluster_mins(df)
            hexels = super()._update_pairings(hexels, func, cluster_mins, hemi)

            # Optional Step 7: Max pool over the denoised hexels.
            if posterior_pooling:
                hexels = self._posterior_pooling(hexels, hemi, func)
        return hexels


class DBSCAN(DenoisingStrategy):
    """
        This approach leverages the algorithm known as Density-based clustering, which focuses on the density of points rather than solely the distance. Intuitively, for each data point, it
        constructs an epsilon-ball neighbourhood and attempts to jump to nearby points and so on. Points that are reached through successive jumps are assigned to the same cluster. To make results more robust,
        it only uses data points where there are at least 2 samples in the neighbourhood, enabling DBSCAN to detect outliers. To make the algorithm tractable, a KD-tree is constructed for efficient look-up of nearby
        points.

        Params
        ------
        For more details, https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html
        eps : int
              the radius of the neighbourhood ball. 
        min_samples : int
                      the number of points required in a neighbourhood for it to be considered significant.
        metric : str
                 the metric used to calculate the distance between points
        metric_params : dict
                        the parameters input to the metric. For instance, if you use Gaussian, you will have to supply the bandwidth parameter.
        algorithm : str
                    the algorithm used to find nearby points. 'auto' chooses the best one itself.
        leaf_size : int
                    leaf size of the KD-tree used for look-up of nearby points. This primarily influences the latency. 
        n_jobs : int
                 the number of processors to use. -1 means use all available ones
    """
    def __init__(self, **kwargs):
        eps = 10 if 'eps' not in kwargs.keys() else kwargs['eps']
        min_samples = 2 if 'min_samples' not in kwargs.keys() else kwargs['min_samples']
        metric = 'euclidean' if 'metric' not in kwargs.keys() else kwargs['metric']
        metric_params = None if 'metric_params' not in kwargs.keys() else kwargs['metric_params']
        algorithm = 'auto' if 'algorithm' not in kwargs.keys() else kwargs['algorithm']
        leaf_size = 30 if 'leaf_size' not in kwargs.keys() else kwargs['leaf_size']
        n_jobs = -1 if 'n_jobs' not in kwargs.keys() else kwargs['n_jobs']

        invalid = False
        if not (isinstance(eps, int) or isinstance(eps, float)) or isinstance(eps, bool):
            print('Epsilon must be of numeric type.')
            invalid = True
        if not isinstance(min_samples, int) or isinstance(min_samples, bool):
            print('Min samples must be of type integer.')
            invalid = True
        if not isinstance(metric, str):
            print('Metric must be a string. It should be one from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances')
            invalid = True
        if metric_params is not None and not isinstance(metric_params, dict):
            print('Metric params must be a dict or None.')
            invalid = True
        if algorithm not in ['auto', 'ball_tree', 'kd_tree', 'brute']:
            print('Algorithm must be one of {auto, ball_tree, kd_tree, brute}')
            invalid = True
        if not isinstance(leaf_size, int) or isinstance(leaf_size, bool):
            print('leaf_size must be of type int.')
            invalid = True
        if not isinstance(n_jobs, int) or isinstance(n_jobs, bool):
            print('The number of jobs must be of type int.')
            invalid = True

        if invalid:
            raise ValueError

        self._clusterer = DBSCAN_(eps=eps, 
                                  min_samples=min_samples,
                                  metric=metric,
                                  metric_params=metric_params,
                                  algorithm=algorithm,
                                  leaf_size=leaf_size,
                                  n_jobs=n_jobs)


class MeanShift(DenoisingStrategy):
    """
        The mean shift algorithm is an improved variant of k-means that does not require the number of clusters to be
        prespecified in advance. 

        https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html for more details.

        Params
        ------
        cluster_all : bool, default = False
                      whether to exclude anomalies or to cluster all points
        bandwidth : float, default = None
                    the bandwidth of the flat kernel used.
        seeds : list, default = None
                the seeds used to initialise the kernel. If none, it estimates it.
        min_bin_freq : int, default = 2
                       the number of points in a bin before it can be considered significant
        n_jobs : int, default = -1
                 the number of processors used. -1 means use all available processors.
    """
    def __init__(self, **kwargs):
        cluster_all = False if 'cluster_all' not in kwargs.keys() else kwargs['cluster_all']
        bandwidth = 30 if 'bandwidth' not in kwargs.keys() else kwargs['bandwidth']
        seeds = None if 'seeds' not in kwargs.keys() else kwargs['seeds']
        min_bin_freq = 2 if 'min_bin_freq' not in kwargs.keys() else kwargs['min_bin_freq']
        n_jobs = -1 if 'n_jobs' not in kwargs.keys() else kwargs['n_jobs']
        
        invalid = False
        if not isinstance(cluster_all, bool):
            print('Cluster_all must be of type bool.')
            invalid = True
        if (not isinstance(bandwidth, float) and not isinstance(bandwidth, int) and bandwidth is not None) or isinstance(bandwidth, bool):
            print('bandwidth must be None or float.')
            invalid = True
        if not isinstance(seeds, list) and seeds is not None:
            print('Seeds must be a list or None.')
            invalid = True
        if not isinstance(min_bin_freq, int or isinstance(min_bin_freq, bool)):
            print('Mininum bin frequency must be of type int.')
            invalid = True
        if not isinstance(n_jobs, int) or isinstance(n_jobs, bool):
            print('Number of jobs must be of type int.')
            invalid = True

        if invalid:
            raise ValueError

        self._clusterer = MeanShift_(bandwidth=bandwidth, 
                                     seeds=seeds,
                                     min_bin_freq=min_bin_freq,
                                     cluster_all=cluster_all,
                                     n_jobs=n_jobs)


